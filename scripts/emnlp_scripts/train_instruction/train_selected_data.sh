# python custom_train_alpaca.py --train_instruction --model_key TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\
#     --lr 2e-5 --batch_size 16 --max_length 256 --epochs 10\
#     --train_lora --lora_rank 128 --lora_alpha 512\
#     --strategy auto --devices 0 1 2 --runs 1 --precision "bf16-true" --accumulate 1 --save_every_epoch --save_name selected_0.05\
#     --task_idxes 1 35 66 3 2 15 34 67 14 7 49 8 20 1074 869 910 5 736 6 394 58 11 16 1346 97 1433 687 1359 906 995 707 615 1191 439 4 860 1114 249 949 1544 674 299 36 1373 370 354 962 1415 1474 108 1648 62 785 1480 609 440 125 1509 1293 352 1043 209 899 1499 112 1647 1530 53 95 516 1417 1392 447 1014 811 236 311 717 239 1387 1193 515 1483 427 37 400


# python custom_train_alpaca.py --train_instruction --model_key TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\
#     --lr 2e-5 --batch_size 16 --max_length 256 --epochs 10\
#     --train_lora --lora_rank 128 --lora_alpha 512\
#     --strategy auto --devices 0 1 2 --runs 1 --precision "bf16-true" --accumulate 1 --save_every_epoch --save_name selected_0.10\
#     --task_idxes 1 35 66 3 2 15 34 67 14 7 49 8 20 1074 869 910 5 736 6 394 58 11 16 1346 97 1433 687 1359 906 995 707 615 1191 439 4 860 1114 249 949 1544 674 299 36 1373 370 354 962 1415 1474 108 1648 62 785 1480 609 440 125 1509 1293 352 1043 209 899 1499 112 1647 1530 53 95 516 1417 1392 447 1014 811 236 311 717 239 1387 1193 515 1483 427 37 400 1016 1264 52 1507 1554 874 167 887 1535 448 1098 1292 1141 219 337 1384 1534 377 807 1375 473 1288 50 1585 829 276 883 801 1518 1614 145 102 1048 588 245 45 1413 362 295 1170 1546 796 1003 511 1254 1301 65 205 1338 75 1577 945 1628 1314 334 1601 1662 1630 1671 744 560 46 644 1603 1593 212 1407 1517 142 1461 709 41 1689 1177 415 312 432 675 925 1350 1623 1613 375 710 1410 640

# python custom_train_alpaca.py --train_instruction --model_key TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\
#     --lr 2e-5 --batch_size 16 --max_length 256 --epochs 10\
#     --train_lora --lora_rank 128 --lora_alpha 512\
#     --strategy auto --devices 0 1 2 --runs 1 --precision "bf16-true" --accumulate 1 --save_every_epoch --save_name selected_0.15_new\
#     --load_model_dir Instruction__TinyLlama-TinyLlama-1.1B-intermediate-step-1431k-3T_lora_r_128_selected_0.15_run_0/epoch_epoch=1\
#     --task_idxes 1 35 66 3 2 15 34 67 14 7 49 8 20 1074 869 910 5 736 6 394 58 11 16 1346 97 1433 687 1359 906 995 707 615 1191 439 4 860 1114 249 949 1544 674 299 36 1373 370 354 962 1415 1474 108 1648 62 785 1480 609 440 125 1509 1293 352 1043 209 899 1499 112 1647 1530 53 95 516 1417 1392 447 1014 811 236 311 717 239 1387 1193 515 1483 427 37 400 1016 1264 52 1507 1554 874 167 887 1535 448 1098 1292 1141 219 337 1384 1534 377 807 1375 473 1288 50 1585 829 276 883 801 1518 1614 145 102 1048 588 245 45 1413 362 295 1170 1546 796 1003 511 1254 1301 65 205 1338 75 1577 945 1628 1314 334 1601 1662 1630 1671 744 560 46 644 1603 1593 212 1407 1517 142 1461 709 41 1689 1177 415 312 432 675 925 1350 1623 1613 375 710 1410 640 1364 422 1462 39 725 561 914 501 952 1707 1181 959 203 724 301 1248 358 664 48 708 794 232 612 43 513 469 216 1145 336 47 1257 1389 1161 1281 1724 150 489 1227 1556 466 1032 56 1272 880 1075 1626 355 1192 958 293 1200 376 1065 255 783 1094 1068 1283 1247 1705 128 1026 269 719 533 1622 40 576 566 1637 13 320 1652 433 1706 313 1055 541 1083 428 1363 626 1618 1710 1366 1485 1490

python custom_train_alpaca.py --train_instruction --model_key TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\
    --lr 2e-5 --batch_size 16 --max_length 256 --epochs 10\
    --train_lora --lora_rank 128 --lora_alpha 512\
    --strategy auto --devices 0 1 --runs 1 --precision "bf16-true" --accumulate 1 --save_every_epoch --save_name selected_0.20\
    --task_idxes 1 35 66 3 2 15 34 67 14 7 49 8 20 1074 869 910 5 736 6 394 58 11 16 1346 97 1433 687 1359 906 995 707 615 1191 439 4 860 1114 249 949 1544 674 299 36 1373 370 354 962 1415 1474 108 1648 62 785 1480 609 440 125 1509 1293 352 1043 209 899 1499 112 1647 1530 53 95 516 1417 1392 447 1014 811 236 311 717 239 1387 1193 515 1483 427 37 400 1016 1264 52 1507 1554 874 167 887 1535 448 1098 1292 1141 219 337 1384 1534 377 807 1375 473 1288 50 1585 829 276 883 801 1518 1614 145 102 1048 588 245 45 1413 362 295 1170 1546 796 1003 511 1254 1301 65 205 1338 75 1577 945 1628 1314 334 1601 1662 1630 1671 744 560 46 644 1603 1593 212 1407 1517 142 1461 709 41 1689 1177 415 312 432 675 925 1350 1623 1613 375 710 1410 640 1364 422 1462 39 725 561 914 501 952 1707 1181 959 203 724 301 1248 358 664 48 708 794 232 612 43 513 469 216 1145 336 47 1257 1389 1161 1281 1724 150 489 1227 1556 466 1032 56 1272 880 1075 1626 355 1192 958 293 1200 376 1065 255 783 1094 1068 1283 1247 1705 128 1026 269 719 533 1622 40 576 566 1637 13 320 1652 433 1706 313 1055 541 1083 428 1363 626 1618 1710 1366 1485 1490 857 527 388 632 729 793 1651 1545 1197 267 1356 214 716 77 1458 681 12 967 368 905 1560 1102 1249 1505 134 438 171 1019 1605 653 1610 611 120 431 1277 1080 1060 1700 1528 909 1665 319 187 1087 289 1190 243 1591 660 649 1230 454 412 122 455 1678 854 385 1284 42 215 1430 878 229 1685 425 817 1383 1321 976 495 940 1620 715 1155 896 531 1064 1657 1708 1520 888 1382 1454 152 581


python custom_train_alpaca.py --train_instruction --model_key TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\
    --lr 2e-5 --batch_size 16 --max_length 256 --epochs 10\
    --train_lora --lora_rank 128 --lora_alpha 512\
    --strategy auto --devices 0 1 --runs 1 --precision "bf16-true" --accumulate 1 --save_every_epoch --save_name toxigen_selected_0.05\
    --task_idxes 26 25 57 136 1233 487 1420 501 770 1500 895 1243 1413 336 291 1007 1696 497 1219 587 1107 1 563 91 1544 1475 1126 191 903 646 1546 137 180 82 704 1379 611 998 1404 540 1109 1662 576 612 1441 999 551 327 425 1507 105 1385 293 352 589 1277 605 273 958 239 766 288 782 645 996 1012 648 172 1494 600 1445 429 510 1547 1569 1074 312 434 636 456 1314 975 1477 1424 979 1076


python custom_train_alpaca.py --train_instruction --model_key TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\
    --lr 2e-5 --batch_size 16 --max_length 256 --epochs 10\
    --train_lora --lora_rank 128 --lora_alpha 512\
    --strategy auto --devices 0 1 --runs 1 --precision "bf16-true" --accumulate 1 --save_every_epoch --save_name toxigen_selected_0.10\
    --task_idxes 26 25 57 136 1233 487 1420 501 770 1500 895 1243 1413 336 291 1007 1696 497 1219 587 1107 1 563 91 1544 1475 1126 191 903 646 1546 137 180 82 704 1379 611 998 1404 540 1109 1662 576 612 1441 999 551 327 425 1507 105 1385 293 352 589 1277 605 273 958 239 766 288 782 645 996 1012 648 172 1494 600 1445 429 510 1547 1569 1074 312 434 636 456 1314 975 1477 1424 979 1076 519 1689 1128 444 997 1439 1003 772 1476 1255 838 300 1422 722 665 1086 1367 188 878 802 833 1588 231 579 1338 556 1574 1077 672 197 472 1621 639 123 1061 850 1394 125 1038 69 1127 1016 720 1260 199 592 785 1532 1548 1102 1142 1228 241 1463 1627 1403 697 1629 1495 1371 1114 234 1542 955 1188 393 1554 1529 469 808 1659 1468 674 303 548 526 1526 1117 657 1251 926 1215 1362 1294 1496 834


python custom_train_alpaca.py --train_instruction --model_key TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\
    --lr 2e-5 --batch_size 16 --max_length 256 --epochs 10\
    --train_lora --lora_rank 128 --lora_alpha 512\
    --strategy auto --devices 0 1 --runs 1 --precision "bf16-true" --accumulate 1 --save_every_epoch --save_name toxigen_selected_0.15\
    --task_idxes 26 25 57 136 1233 487 1420 501 770 1500 895 1243 1413 336 291 1007 1696 497 1219 587 1107 1 563 91 1544 1475 1126 191 903 646 1546 137 180 82 704 1379 611 998 1404 540 1109 1662 576 612 1441 999 551 327 425 1507 105 1385 293 352 589 1277 605 273 958 239 766 288 782 645 996 1012 648 172 1494 600 1445 429 510 1547 1569 1074 312 434 636 456 1314 975 1477 1424 979 1076 519 1689 1128 444 997 1439 1003 772 1476 1255 838 300 1422 722 665 1086 1367 188 878 802 833 1588 231 579 1338 556 1574 1077 672 197 472 1621 639 123 1061 850 1394 125 1038 69 1127 1016 720 1260 199 592 785 1532 1548 1102 1142 1228 241 1463 1627 1403 697 1629 1495 1371 1114 234 1542 955 1188 393 1554 1529 469 808 1659 1468 674 303 548 526 1526 1117 657 1251 926 1215 1362 1294 1496 834 1261 1176 681 908 383 158 821 1717 1405 1368 1268 570 1339 1335 244 1214 419 350 1182 464 1469 1605 1370 1656 1380 1677 1317 1253 195 1381 1360 683 1173 138 989 1147 321 356 1725 723 957 1035 578 538 194 19 1668 810 698 1539 893 853 1530 474 142 748 97 1599 678 883 1304 1083 140 1415 247 147 344 509 318 382 373 762 24 933 799 417 917 604 677 840 1708 565 1171 438 1010 1299 477


python custom_train_alpaca.py --train_instruction --model_key TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\
    --lr 2e-5 --batch_size 16 --max_length 256 --epochs 10\
    --train_lora --lora_rank 128 --lora_alpha 512\
    --strategy auto --devices 0 1 --runs 1 --precision "bf16-true" --accumulate 1 --save_every_epoch --save_name toxigen_selected_0.20\
    --task_idxes 26 25 57 136 1233 487 1420 501 770 1500 895 1243 1413 336 291 1007 1696 497 1219 587 1107 1 563 91 1544 1475 1126 191 903 646 1546 137 180 82 704 1379 611 998 1404 540 1109 1662 576 612 1441 999 551 327 425 1507 105 1385 293 352 589 1277 605 273 958 239 766 288 782 645 996 1012 648 172 1494 600 1445 429 510 1547 1569 1074 312 434 636 456 1314 975 1477 1424 979 1076 519 1689 1128 444 997 1439 1003 772 1476 1255 838 300 1422 722 665 1086 1367 188 878 802 833 1588 231 579 1338 556 1574 1077 672 197 472 1621 639 123 1061 850 1394 125 1038 69 1127 1016 720 1260 199 592 785 1532 1548 1102 1142 1228 241 1463 1627 1403 697 1629 1495 1371 1114 234 1542 955 1188 393 1554 1529 469 808 1659 1468 674 303 548 526 1526 1117 657 1251 926 1215 1362 1294 1496 834 1261 1176 681 908 383 158 821 1717 1405 1368 1268 570 1339 1335 244 1214 419 350 1182 464 1469 1605 1370 1656 1380 1677 1317 1253 195 1381 1360 683 1173 138 989 1147 321 356 1725 723 957 1035 578 538 194 19 1668 810 698 1539 893 853 1530 474 142 748 97 1599 678 883 1304 1083 140 1415 247 147 344 509 318 382 373 762 24 933 799 417 917 604 677 840 1708 565 1171 438 1010 1299 477 827 1008 56 761 1112 104 1600 739 726 881 1053 950 1006 1485 1124 1672 621 855 771 1274 280 1414 934 847 1556 1172 676 591 398 637 1501 1099 476 797 517 1377 399 77 1204 1636 289 745 1509 900 1462 809 562 832 1676 491 828 685 1567 1634 1454 1503 973 60 98 1002 230 1637 1297 88 1345 545 1671 492 1387 965 466 1388 983 885 963 547 823 130 668 894 1283 133 87 1541 858 1282
